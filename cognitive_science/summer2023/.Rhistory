legend.direction = "vertical",# Ensure items within each legend are vertical
legend.title = element_text(size = 28),
legend.text = element_text(size = 24),
panel.grid.minor = element_blank(),
plot.title = element_text(hjust = 0.5)
) +
guides(
# Ensure the Suspicion Level legend (color) is vertical
color = guide_legend(ncol = 1),
# For linetype (Lexical, Syntactic, Semantic), set a vertical layout and enlarge keys
linetype = guide_legend(
ncol = 1,
override.aes = list(size = 2.5),  # Increase line thickness in the legend keys
keywidth = unit(3, "lines"),      # Increase the width of the legend keys
keyheight = unit(2, "lines")      # Increase the height of the legend keys
)
)
# To check the plot
print(p)
# ggsave("alignment_plot2.png", plot = p, width = 800/72, height = 800/72, units = "in", dpi = 300)
ggsave("alignment_plot2.png", plot = p, width = 800/72, height = 800/72, units = "in", dpi = 300)
# Load libraries
# Create data
time_points <- c("0:00", "0:30", "1:00", "1:30", "2:00")
create_data <- function(time_points) {
data.frame(
timePoint = rep(time_points, each = 6),
suspicion = rep(c("High", "Low"), each = 3, times = length(time_points)),
type = rep(c("Lexical", "Syntactic", "Semantic"), times = 2 * length(time_points)),
value = c(
# High suspicion values
0.2, 0.3, 0.25,  # 0:00
0.15, 0.25, 0.2,  # 0:30
0.1, 0.2, 0.15,   # 1:00
0.25, 0.15, 0.2,  # 1:30
0.2, 0.1, 0.15,   # 2:00
# Low suspicion values
0.4, 0.5, 0.45,   # 0:00
0.5, 0.6, 0.55,   # 0:30
0.6, 0.7, 0.65,   # 1:00
0.55, 0.65, 0.7,  # 1:30
0.65, 0.75, 0.7   # 2:00
)
)
}
data <- create_data(time_points)
# Convert timePoint to a continuous numeric variable (time in minutes)
data <- data %>%
mutate(time = as.numeric(sub(":.*", "", timePoint)) +
as.numeric(sub(".*:", "", timePoint)) / 60)
# Interpolate the data for smoother curves using a spline
# (This creates 200 points for each group so that the lines look smooth.)
data_smooth <- data %>%
group_by(suspicion, type) %>%
do({
data.frame(
time_interp = seq(min(.$time), max(.$time), length.out = 200),
value_interp = spline(.$time, .$value, n = 200)$y
)
}) %>%
ungroup()
# Create the plot using the interpolated data
ggplot(data_smooth, aes(x = time_interp, y = value_interp,
color = suspicion,
linetype = type,
group = interaction(suspicion, type))) +
geom_line(size = 1) +
scale_color_manual(values = c("High" = "#f56565", "Low" = "#4299e1")) +
scale_linetype_manual(values = c("Lexical" = "dashed",
"Syntactic" = "dotted",
"Semantic" = "solid")) +
# Set custom breaks and labels for the continuous x-axis
scale_x_continuous(breaks = c(0, 0.5, 1, 1.5, 2),
labels = c("0:00", "0:30", "1:00", "1:30", "2:00"),
expand = c(0.01, 0.01)) +
labs(x = "Time into Call",
y = "Alignment Score",
color = "Suspicion Level",
linetype = "Alignment Type") +
theme_minimal(base_size = 14) +
theme(
# axis.title = element_text(size = 28),
# axis.text = element_text(size = 24),
legend.position = "bottom",
legend.box = "vertical",
panel.grid.minor = element_blank(),
plot.title = element_text(hjust = 0.5)
)
# ggsave("alignment_plot.png", width = 10, height = 7, dpi = 300, units = "in")
# Create the data frame
model_data <- data.frame(
Model = c("Stacked Ensemble", "Naive Bayes", "SVM", "LightGBM",
"Elastic Net", "XGBoost", "Random Forest", "kNN"),
GeMAPS = c(0.55, 0.64, 0.72, 0.72, 0.61, 0.70, 0.68, 0.72),
IS09 = c(0.58, 0.63, 0.65, 0.68, 0.69, 0.69, 0.74, 0.73)
)
# Reshape data to long format
model_data_long <- model_data %>%
pivot_longer(cols = c(GeMAPS, IS09),
names_to = "Feature_Set",
values_to = "Accuracy")
# Create the heatmap
heatmap_plot <- ggplot(model_data_long,
aes(x = Feature_Set, y = Model, fill = Accuracy)) +
geom_tile(color = "white", linewidth = 0.5) +
geom_text(aes(label = sprintf("%.2f", Accuracy)),
size = 20,  # Increased text size
color = ifelse(model_data_long$Accuracy >= 0.65, "white", "black")) +
scale_fill_gradient(low = "#fecaca", high = "#dc2626",  # Using specified red color scheme
limits = c(0.5, 0.8)) +
scale_y_discrete(limits = rev) +  # Reverse model order to match original
labs(title = "Model Balanced Accuracy",
fill = "Accuracy") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 12, face = "bold"),
axis.text.y = element_text(size = 12),
plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
legend.title = element_text(size = 12),
legend.text = element_text(size = 10),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.title = element_blank(),
legend.position = "right"
)
# ggsave("model_accuracy_heatmap.png",
#        plot = heatmap_plot,
#        width = 10,
#        height = 12,
#        units = "in",
#        dpi = 300,
#        bg = "white")  # Ensures white background
heatmap_plot
# Create a data frame with suspicion ratings
data <- data.frame(
Category = factor(c("Laypeople", "Laypeople", "Police", "Police"),
levels = c("Laypeople", "Police")),
Gender = factor(c("Male", "Female", "Male", "Female")),
Mean_Suspicion = c(1.85, 1.69, 4.34, 3.86),
SD = c(1.31, 1.31, 1.13, 1.13)
)
# Create annotation data frame for the asterisk - now including all necessary aesthetics
sig_annotation <- data.frame(
Category = "Police",
Gender = "Male",  # Added this to match aesthetics
Mean_Suspicion = max(data$Mean_Suspicion[data$Category == "Police"]) +
max(data$SD[data$Category == "Police"]) + 0.3
)
# Create the enhanced bar chart
bar_plot <- ggplot(data, aes(x = Category, y = Mean_Suspicion,
fill = Gender,
group = interaction(Category, Gender))) +
# Add bars with closer grouping
geom_bar(stat = "identity",
position = position_dodge(width = 0.8),
width = 0.7) +
# Add error bars
geom_errorbar(aes(ymin = Mean_Suspicion - SD,
ymax = Mean_Suspicion + SD),
position = position_dodge(width = 0.8),
width = 0.2,
color = "#2d3748") +
# Add significance asterisk
geom_text(data = sig_annotation,
aes(y = Mean_Suspicion, label = "*"),
position = position_dodge(width = 0.4),  # Center between bars
size = 14,
show.legend = FALSE) +
# Custom fill colors for gender
scale_fill_manual(values = c(
"Male" = "#4299e1",    # Medium Blue
"Female" = "#f87171"   # Light Red (softer than dark red)
)) +
# Customize labels
labs(
title = "Suspicion Ratings by Group and Gender",
y = "Average Suspicion Rating",
x = NULL,
fill = "Gender"
) +
# Set y-axis limits and breaks - adjusted to accommodate asterisk
scale_y_continuous(
limits = c(0, 6.5),
breaks = seq(0, 6, by = 1),
expand = expansion(mult = c(0, 0.1))
) +
# Customize theme
theme_minimal() +
theme(
plot.title = element_text(
size = 16,
face = "bold",
color = "#2d3748",
margin = margin(b = 20)
),
axis.title.y = element_text(
size = 12,
color = "#2d3748",
margin = margin(r = 10)
),
axis.text = element_text(
size = 11,
color = "#4a5568"
),
panel.grid.major.y = element_line(
color = "#e2e8f0",
linewidth = 0.3
),
panel.grid.major.x = element_blank(),
panel.grid.minor = element_blank(),
legend.position = "right",
legend.title = element_text(face = "bold"),
plot.margin = margin(t = 20, r = 20, b = 20, l = 20)
)
bar_plot
# ggsave("suspicion_ratings.png",
#        plot = bar_plot,
#        width = 10,
#        height = 8,
#        units = "in",
#        dpi = 500,      # High DPI for sharp bars and text
#        bg = "white")   # Ensure white background
# Create the data frame
error_metrics <- data.frame(
Metric = c("Word Error Rate (WER)", "Diarization Error Rate (DER)"),
Value = c(0.2794, 0.4727),
Description = c("Transcription accuracy", "Speaker identification accuracy")
)
# Create the adjusted plot
ggplot(error_metrics, aes(x = Metric, y = Value)) +
geom_bar(stat = "identity",
fill = "#f87171",       # Updated to light red matching the enhanced chart
width = 0.7,            # Increased bar width for closer grouping
position = position_dodge(width = 0.8)) +
geom_text(aes(label = sprintf("%.1f%%", Value * 100)),
vjust = -0.5,
color = "#2d3748",
size = 6,
position = position_dodge(width = 0.8)) +
scale_y_continuous(limits = c(0, 1),
labels = scales::percent,
expand = expansion(mult = c(0, 0.1))) +
labs(title = "911 Call Analysis System Performance",
subtitle = "Key Error Metrics",
x = NULL,
y = "Error Rate") +
theme_minimal() +
theme(
plot.title = element_text(face = "bold", size = 14, color = "#2d3748", margin = margin(b = 20)),
plot.subtitle = element_text(color = "#4a5568"),
axis.text.x = element_text(angle = 0, hjust = 0.5),
axis.text = element_text(size = 11, color = "#4a5568"),
axis.title.y = element_text(size = 12, color = "#2d3748", margin = margin(r = 10)),
panel.grid.major.x = element_blank(),
panel.grid.minor = element_blank()
)
# Create the data frame
error_metrics <- data.frame(
Metric = c("Word Error Rate (WER)", "Diarization Error Rate (DER)"),
Value = c(0.2794, 0.4727),
Description = c("Transcription accuracy", "Speaker identification accuracy")
)
# Create the adjusted plot
ggplot(error_metrics, aes(x = Metric, y = Value)) +
geom_bar(stat = "identity",
fill = "#f87171",       # Updated to light red matching the enhanced chart
width = 0.7,            # Increased bar width for closer grouping
position = position_dodge(width = 0.8)) +
geom_text(aes(label = sprintf("%.1f%%", Value * 100)),
vjust = -0.5,
color = "#2d3748",
size = 6,
position = position_dodge(width = 0.8)) +
scale_y_continuous(limits = c(0, 1),
labels = scales::percent,
expand = expansion(mult = c(0, 0.1))) +
labs(title = "911 Call Analysis System Performance",
subtitle = "Key Error Metrics",
x = NULL,
y = "Error Rate") +
theme_minimal() +
theme(
plot.title = element_text(face = "bold", size = 14, color = "#2d3748", margin = margin(b = 20)),
plot.subtitle = element_text(color = "#4a5568"),
axis.text.x = element_text(angle = 0, hjust = 0.5),
axis.text = element_text(size = 11, color = "#4a5568"),
axis.title.y = element_text(size = 12, color = "#2d3748", margin = margin(r = 10)),
panel.grid.major.x = element_blank(),
panel.grid.minor = element_blank()
)
ggsave("911-calls3.png",
width = 6,
height = 8,
units = "in",
dpi = 300)      # High DPI for sharp bars and text
# bg = "white")   # Ensure white background
library(tidyverse)
# Read the datasets
# Note: Replace file paths with your actual file paths
workload_data <- read_csv("faculty_workload.csv")
self_reported <- read_csv("self_reported_releases.csv")
# Function to split faculty names
split_faculty_name <- function(name) {
# Handle cases where name might be NA
if (is.na(name)) {
return(list(last_name = NA, first_name = NA))
}
# Split on the last space to handle middle names correctly
parts <- str_split(str_trim(name), " ")[[1]]
list(
last_name = parts[length(parts)],
first_name = str_c(parts[-length(parts)], collapse = " ")
)
}
# Process self-reported data
processed_self_reported <- self_reported %>%
# Split faculty names
mutate(
name_parts = map(.$`Faculty Member's Name`, split_faculty_name),
Last_Name = map_chr(name_parts, "last_name"),
First_Name = map_chr(name_parts, "first_name")
) %>%
# Count total releases
mutate(
Self_Reported_Course_Releases = `How many releases are you reporting for the semester/year indicated above?`
) %>%
# Combine release reasons
mutate(
Self_Reported_Release_Reasons = pmap_chr(
list(
`Release 1: What is the documented reason for your release(s)?`,
`Release 2: What is the documented reason for your release(s)?`,
`Release 3: What is the documented reason for your release(s)?`,
`Release 4: What is the documented reason for your release(s)?`
),
function(...) {
reasons <- c(...)
reasons <- reasons[!is.na(reasons)]
if (length(reasons) == 0) return(NA_character_)
paste(reasons, collapse = "; ")
}
)
) %>%
# Select and rename relevant columns
select(
Last_Name,
First_Name,
Email = `Email Address`,
Semester = `Semester reporting`,
Year,
Self_Reported_Course_Releases,
Self_Reported_Release_Reasons
)
# Aggregate multiple entries per faculty member
aggregated_self_reported <- processed_self_reported %>%
group_by(Last_Name, First_Name) %>%
summarise(
Email = first(Email),
Semesters_Reported = paste(unique(paste(Semester, Year)), collapse = "; "),
Total_Self_Reported_Releases = sum(Self_Reported_Course_Releases, na.rm = TRUE),
All_Release_Reasons = paste(unique(Self_Reported_Release_Reasons[!is.na(Self_Reported_Release_Reasons)]),
collapse = "; "),
.groups = "drop"
)
# Clean and standardize workload data
cleaned_workload <- workload_data %>%
rename(
Last_Name = `Last Name`,
First_Name = `First Name`
) %>%
mutate(
Last_Name = str_trim(Last_Name),
First_Name = str_trim(First_Name)
)
# Merge datasets
final_dataset <- cleaned_workload %>%
left_join(
aggregated_self_reported,
by = c("Last_Name", "First_Name")
) %>%
# Organize columns in logical order
select(
School,
Last_Name,
First_Name,
Email,
`Title of Work`,
`Number of Course Releases`,
`Assignment for AY or Semester`,
`Workload Modification`,
Semesters_Reported,
Total_Self_Reported_Releases,
All_Release_Reasons
)
colnames(workload_data)
library(tidyverse)
# Read the datasets
# Note: Replace file paths with your actual file paths
workload_data <- read_csv("faculty_workload.csv")
self_reported <- read_csv("self_reported_releases.csv")
# Function to split faculty names
split_faculty_name <- function(name) {
# Handle cases where name might be NA
if (is.na(name)) {
return(list(last_name = NA, first_name = NA))
}
# Split on the last space to handle middle names correctly
parts <- str_split(str_trim(name), " ")[[1]]
list(
last_name = parts[length(parts)],
first_name = str_c(parts[-length(parts)], collapse = " ")
)
}
# Process self-reported data
processed_self_reported <- self_reported %>%
# Split faculty names
mutate(
name_parts = map(.$`Faculty Member's Name`, split_faculty_name),
Last_Name = map_chr(name_parts, "last_name"),
First_Name = map_chr(name_parts, "first_name")
) %>%
# Count total releases
mutate(
Self_Reported_Course_Releases = `How many releases are you reporting for the semester/year indicated above?`
) %>%
# Combine release reasons
mutate(
Self_Reported_Release_Reasons = pmap_chr(
list(
`Release 1: What is the documented reason for your release(s)?`,
`Release 2: What is the documented reason for your release(s)?`,
`Release 3: What is the documented reason for your release(s)?`,
`Release 4: What is the documented reason for your release(s)?`
),
function(...) {
reasons <- c(...)
reasons <- reasons[!is.na(reasons)]
if (length(reasons) == 0) return(NA_character_)
paste(reasons, collapse = "; ")
}
)
) %>%
# Select and rename relevant columns
select(
Last_Name,
First_Name,
Email = `Email Address`,
Semester = `Semester reporting`,
Year,
Self_Reported_Course_Releases,
Self_Reported_Release_Reasons
)
# Aggregate multiple entries per faculty member
aggregated_self_reported <- processed_self_reported %>%
group_by(Last_Name, First_Name) %>%
summarise(
Email = first(Email),
Semesters_Reported = paste(unique(paste(Semester, Year)), collapse = "; "),
Total_Self_Reported_Releases = sum(Self_Reported_Course_Releases, na.rm = TRUE),
All_Release_Reasons = paste(unique(Self_Reported_Release_Reasons[!is.na(Self_Reported_Release_Reasons)]),
collapse = "; "),
.groups = "drop"
)
# Clean and standardize workload data
# Print column names to help diagnose issues
print("Available columns in workload_data:")
print(colnames(workload_data))
cleaned_workload <- workload_data %>%
rename(
Last_Name = `Last Name`,
First_Name = `First Name`
) %>%
mutate(
Last_Name = str_trim(Last_Name),
First_Name = str_trim(First_Name)
)
# Merge datasets
final_dataset <- cleaned_workload %>%
left_join(
aggregated_self_reported,
by = c("Last_Name", "First_Name")
) %>%
# Organize columns in logical order
select(
School,
Last_Name,
First_Name,
Email,
`Title of Work`,
`Number of Course Releases`,
`Assignment for AY or Semester`,
`workload modification`,
Semesters_Reported,
Total_Self_Reported_Releases,
All_Release_Reasons
)
# Write the final dataset to CSV
write_csv(final_dataset, "merged_faculty_releases.csv")
# Find faculty in self-reported data who aren't in workload data
missing_faculty <- aggregated_self_reported %>%
anti_join(
cleaned_workload,
by = c("Last_Name", "First_Name")
) %>%
arrange(Last_Name, First_Name)
# Print missing faculty
if (nrow(missing_faculty) > 0) {
cat("\nFaculty members in self-reported data but not in workload data:\n")
print(missing_faculty)
} else {
cat("\nAll self-reported faculty were found in workload data.\n")
}
